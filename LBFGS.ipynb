{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import autograd.numpy as np\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from autograd import grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Intro\n",
    "Actual Newton methods for nonlinear programming problems typically involves an update of a set of optimizatio parameters $x_k$ that looks like:\n",
    "\\begin{equation}\n",
    "    x_{k+1} = x_k + H_k^{-1}\\nabla f(x_k)\n",
    "\\end{equation}\n",
    "where $H_k$ is the hessian of the objective and $\\nabla$ is the Jacobian.\n",
    "\n",
    "The class of quasi-newton methods are small: SR1, BFGS, DRP, Broyden class (all of which are related)\n",
    "\n",
    "## L-BFGS\n",
    "This is probably the state of the art quasi newton. Others include conjugate gradients.\n",
    "\\begin{equation}\n",
    "    B_{k+1} = B_k + \\frac{y_ky_k^T}{y_k^Ts_k} - \\frac{B_ks_ks_k^TB_k^T}{s_k^TB_ks_k}\n",
    "\\end{equation}\n",
    "where $B_k$ is the \"approximate\" Hessian. Even for this \"approximate\" Hessian, the size alone is prohibitive, so a limited memory version of this involves:\n",
    "\n",
    "rank one matrix: one linearly independent row or column (i.e. a scalar is rank one)\n",
    "rank two matrix: two linearly independent rows or columns? (i.e. 2x2 identity amtrix is rank two)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LBFGS (limited) memory\n",
    "So, can we tell what in the BFGS algorithm costs a lot? It's Hk, most likely. LBFGS overcomes his by approximating $Hk$ through it's diagonal only. The algorithm we attempt to implement will be based off the Wikipedia description, which uses a two loop recursion.\n",
    "\n",
    "THe LBFGS requires us to store multiple gradients seen sequentially during the optimization (so it doesn't start until we've seen k gradients for example)\n",
    "\n",
    "So, the L-BFGS algorithm is just a new way to compute this approximation:\n",
    "\n",
    "$z_k$ is what is used now to denote the final search direction.\n",
    "\n",
    "\\begin{equation}\n",
    "    q= g_k\n",
    "\\end{equation}\n",
    "\n",
    "We further define:\n",
    "\\begin{equation}\n",
    "s_k = x_{k+1}-x_k\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "y_k = g_{k+1}-g_k\n",
    "\\end{equation}\n",
    "\n",
    "I would interpret $s_k$ as a first order difference of the variable and $s_k$ as a first order difference of the gradient.\n",
    "\n",
    "where $g_k$ is the gradient computed a the $kth$ \n",
    "\n",
    "for $ i = k-1, k-2, ... k-m $\n",
    "\n",
    "\\begin{equation}\n",
    "    \\alpha_i = \\rho_i s_i^Tq\n",
    "\\end{equation}\n",
    "where $\\rho_i$ is given by:\n",
    "\\begin{equation}\n",
    "    \\rho_i = \\frac{1}{s_i^T y_i}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "    q = q-\\alpha_iy_i\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we finish this loop to update our $q$, then we compute\n",
    "\\begin{equation}\n",
    "    \\gamma_k = \\frac{s_{k-1}^{T}y_{k-1}}{s_k^Ty_{k-1}}\n",
    "\\end{equation}\n",
    "and we set our first approximation of the Hessian\n",
    "\n",
    "\\begin{equation}\n",
    "    H^{(0)}_k = \\gamma_k I\n",
    "\\end{equation}\n",
    "\n",
    "We are at iteration $k$ and we have $s_k$, which requires $x_{k+1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we finish with one for for loop:\n",
    "\n",
    "for $ i = k-1, k-2, ... k-m $\n",
    "\\begin{equation}\n",
    "    \\beta_i = \\rho_iy_i^Tz\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    z = z+ s_i(\\alpha_i - \\beta_i)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wolfe Searches\n",
    "\n",
    "\n",
    "### Strong Wolfe\n",
    "The strong wolfe conditions appear to require a re-evaluation of the GRADIENT, but it does seem to minimize, in general the number of evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "(100.0, array([-400.,  200.]))\n",
      "0.0 [-2.  0.]\n",
      "0.8862525783330446\n"
     ]
    }
   ],
   "source": [
    "## Use the Rosenbrock function\n",
    "import autograd\n",
    "\n",
    "def rosen2(arr):\n",
    "    return ((1 - arr[0])**2 + 100*(arr[1] - arr[0]**2)**2) \n",
    "    \n",
    "#grad_rosen2 = grad(rosen2)\n",
    "\n",
    "print(rosen2(np.array([1,2])))\n",
    "\n",
    "rosenbrock_with_grad = autograd.value_and_grad(rosen2)\n",
    "\n",
    "rosen2_grad = grad(rosen2);\n",
    "\n",
    "print(rosenbrock_with_grad(np.array([1.,2.])))\n",
    "\n",
    "## minimum is 1, 1\n",
    "print(rosen2(np.array([1.,1.])), rosen2_grad(np.array([0.,0.])))\n",
    "\n",
    "print(rosen2(np.array([0.05865546, 0.00233125])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191.93970384086134\n"
     ]
    }
   ],
   "source": [
    "## multidimensional rosenbrock function\n",
    "\n",
    "def rosen( x ):  # rosen.m\n",
    "        # a sum of squares, so LevMar (scipy.optimize.leastsq) is pretty good\n",
    "    x0 = x[:-1]\n",
    "    x1 = x[1:]\n",
    "    return (np.sum( (1 - x0) **2 )\n",
    "        + 100 * np.sum( (x1 - x0**2) **2 ))\n",
    "\n",
    "print(rosen(np.random.rand(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general rosenbrock is:\n",
    "\\begin{equation}\n",
    "f(\\mathbf{x}) = \\sum_{i}^{N}(100(x_{i+1}-x_i^2)^2 +(1-x_i^2)\n",
    "\\end{equation}\n",
    "\n",
    "The 2D function has a gradient:\n",
    "\\begin{equation}\n",
    "    \\nabla f(x,y) = (-200(y-x^2)x - 2x, 200(y-x^2))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "## other functions\n",
    "def ellipse( x ):\n",
    "    return np.mean( (1 - x) **2 )  + 100 * np.mean( np.diff(x) **2 )\n",
    "\n",
    "#...............................................................................\n",
    "def nesterov( x ):\n",
    "    \"\"\" Nesterov's nonsmooth Chebyshev-Rosenbrock function, Overton 2011 variant 2 \"\"\"\n",
    "    x0 = x[:-1]\n",
    "    x1 = x[1:]\n",
    "    return np.abs( 1 - x[0] ) / 4 \\\n",
    "        + np.sum( np.abs( x1 - 2*abs(x0) + 1 ))\n",
    "\n",
    "#...............................................................................\n",
    "def saddle( x ):\n",
    "    return np.mean( np.diff( x **2 )) \\\n",
    "        + .5 * np.mean( x **4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = rosen;\n",
    "gradf = grad(rosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LBFGS with strong wolfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial ls count:  8\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  2\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "1.5422207654538814e-10 1.5422207654538814e-10 [1.0000108  1.00002099]\n"
     ]
    }
   ],
   "source": [
    "##LBFGS with obj and grad_j provided by autograd\n",
    "from collections import deque\n",
    "ITERS = 20;\n",
    "m = 10; #number of gks and sks that we store\n",
    "\n",
    "\n",
    "n = 2; #dimension of n-dimensional rosenbrock\n",
    "\n",
    "x0 = np.zeros(n)\n",
    "x0 = np.random.rand(n)\n",
    "g0 = np.zeros(n)\n",
    "H0 = np.identity(n); #first approximation of the hessian\n",
    "history = deque(maxlen = m);\n",
    "\n",
    "\n",
    "gradient_history = deque(maxlen = m)\n",
    "sk_history = deque(maxlen = m)\n",
    "yk_history = deque(maxlen = m)\n",
    "\n",
    "Sm = np.zeros((n,m))\n",
    "Ym = np.zeros((n,m))\n",
    "## outer index i\n",
    "\n",
    "gradToler = 1e-10; #% tolerance for the norm of the slope\n",
    "XToler = 1e-10;    #% tolerance for the variables' refinement\n",
    "\n",
    "## RUN 1 ITERATION PRIOR to entering the loop\n",
    "# assumes function is differentiable\n",
    "[f0,g0]=f(x0), gradf(x0); # f0 is evaluation, g0 is the gradient\n",
    "                        # we want to move the gradient eval as a seperate function\n",
    "#print(f0, g0)\n",
    "    \n",
    "# line search\n",
    "# usually line search method only return step size alpha\n",
    "# we return 3 variables to save caculation time.\n",
    "[alpha,f1,g1, eval_count] = strongwolfe(f, gradf,-g0,x0,f0,g0);\n",
    "#alpha,fs, eval_count = weakwolfe();\n",
    "\n",
    "print('initial ls count: ', eval_count)\n",
    "#print(alpha, f1, g1)\n",
    "x1 = x0 - alpha*g0;\n",
    "k =0;\n",
    "\n",
    "for _ in range(ITERS):\n",
    "    \n",
    "    ## use norm tolerance...this prevents the algorithm from having divide by zero errors\n",
    "    fnorm = np.linalg.norm(g0);\n",
    "    if(fnorm < gradToler):\n",
    "        break;\n",
    "    \n",
    "    s0 = x1-x0; #nx1\n",
    "    ##\n",
    "    y0 = g1-g0; #nxn\n",
    "    \n",
    "    ## should this be a vector or not a vector?\n",
    "    gamma_k = np.dot(s0.T,y0)/np.dot(y0,y0)\n",
    "    hdiag = gamma_k*np.identity(n); #diagonal of the hessian, dneominator is scalar, numerator is a vector\n",
    "    p = np.zeros((len(g0),1));\n",
    "    #print(hdiag)\n",
    "    ## UPDATE SEARCH DIRECTION\n",
    "    if(k<m):  ##Cache isn't full\n",
    "        # update S,Y\n",
    "        Sm[:,k] = s0;\n",
    "        Ym[:,k] = y0;\n",
    "        # never forget the minus sign\n",
    "        ## this doesn't seem to recompute any new g0 \n",
    "        p = -getHg_lbfgs(g1,Sm[:,:k],Ym[:,:k],hdiag); \n",
    "        \n",
    "    elif(k>=m): # the cache is full, reupdate the cache\n",
    "        Sm[:,:m-1]=Sm[:,1:m];\n",
    "        Ym[:,:m-1]=Ym[:,1:m];\n",
    "        Sm[:,m-1] = s0;\n",
    "        Ym[:,m-1] = y0;    \n",
    "        # never forget the minus sign\n",
    "        p = -getHg_lbfgs(g1,Sm,Ym,hdiag);\n",
    "   \n",
    "    ## LINE SEARCH: strong wolfe also provides the new gradient at the new point...\n",
    "    # if we use weak wolfe...then what?\n",
    "    [alpha ,fs,gs, eval_count]= strongwolfe(f, gradf,p,x1,f1,g1);\n",
    "    \n",
    "    \n",
    "    print('evals for line search: ', eval_count)\n",
    "    x0 = x1; ## update x0 with x1 the delta\n",
    "    g0 = g1;\n",
    "    \n",
    "    ## make a step\n",
    "    x1 = x1 + alpha*p;\n",
    "    f1 = fs;\n",
    "    g1 = gs;\n",
    "    # save caculation\n",
    "    # [f1,g1]=feval(myFx,x1);\n",
    "    k = k + 1;\n",
    "    \n",
    "print(f1, f(x1), x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lbfgs with weak wolfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial ls count:  22\n",
      "evals for line search:  26\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  2\n",
      "evals for line search:  1\n",
      "evals for line search:  2\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  2\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  2\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  4\n",
      "evals for line search:  1\n",
      "evals for line search:  3\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  2\n",
      "evals for line search:  1\n",
      "evals for line search:  2\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  2\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "evals for line search:  1\n",
      "1.365316217387385e-13 1.365316217387385e-13 [0.99999999 0.99999999 1.         1.         1.         1.\n",
      " 0.99999999 0.99999996 0.9999999  0.9999998 ]\n"
     ]
    }
   ],
   "source": [
    "f = rosen;\n",
    "gradf = grad(f)\n",
    "\n",
    "##LBFGS with obj and grad_j provided by autograd\n",
    "ITERS = 60;\n",
    "m = 10; #number of gks and sks that we store\n",
    "\n",
    "\n",
    "n = 10; #dimension of n-dimensional rosenbrock\n",
    "\n",
    "x0 = np.zeros(n)\n",
    "x0 = np.random.rand(n)\n",
    "g0 = np.zeros(n)\n",
    "H0 = np.identity(n); #first approximation of the hessian\n",
    "history = deque(maxlen = m);\n",
    "\n",
    "\n",
    "gradient_history = deque(maxlen = m)\n",
    "sk_history = deque(maxlen = m)\n",
    "yk_history = deque(maxlen = m)\n",
    "\n",
    "Sm = np.zeros((n,m))\n",
    "Ym = np.zeros((n,m))\n",
    "## outer index i\n",
    "\n",
    "gradToler = 1e-10; #% tolerance for the norm of the slope\n",
    "XToler = 1e-10;    #% tolerance for the variables' refinement\n",
    "\n",
    "## RUN 1 ITERATION PRIOR to entering the loop\n",
    "# assumes function is differentiable\n",
    "[f0,g0]=f(x0), gradf(x0); # f0 is evaluation, g0 is the gradient\n",
    "                        # we want to move the gradient eval as a seperate function\n",
    "#print(f0, g0)\n",
    "    \n",
    "# line search\n",
    "# usually line search method only return step size alpha\n",
    "# we return 3 variables to save caculation time.\n",
    "# [alpha,f1 eval_count] = strongwolfe(f, gradf,-g0,x0,f0,g0);\n",
    "\n",
    "alpha,fs, eval_count =  weak_wolfe(f, -g0, x0, f0, g0, fac = 0.9);\n",
    "\n",
    "print('initial ls count: ', eval_count)\n",
    "#print(alpha, f1, g1)\n",
    "x1 = x0 - alpha*g0;\n",
    "k =0;\n",
    "\n",
    "for _ in range(ITERS):\n",
    "    \n",
    "    ## use norm tolerance...this prevents the algorithm from having divide by zero errors\n",
    "    fnorm = np.linalg.norm(g0);\n",
    "    if(fnorm < gradToler):\n",
    "        break;\n",
    "    \n",
    "    s0 = x1-x0; #nx1\n",
    "    g1 = gradf(x1);\n",
    "    \n",
    "    y0 = g1-g0; #nxn\n",
    "    \n",
    "    \n",
    "    ## should this be a vector or not a vector?\n",
    "    gamma_k = np.dot(s0.T,y0)/np.dot(y0,y0)\n",
    "    hdiag = gamma_k*np.identity(n); #diagonal of the hessian, dneominator is scalar, numerator is a vector\n",
    "    p = np.zeros((len(g0),1));\n",
    "    #print(hdiag)\n",
    "    ## UPDATE SEARCH DIRECTION\n",
    "    if(k<m):  ##Cache isn't full\n",
    "        # update S,Y\n",
    "        Sm[:,k] = s0;\n",
    "        Ym[:,k] = y0;\n",
    "        # never forget the minus sign\n",
    "        ## this doesn't seem to recompute any new g0 \n",
    "        p = -getHg_lbfgs(g1,Sm[:,:k],Ym[:,:k],hdiag); \n",
    "    elif(k>=m): # the cache is full, reupdate the cache\n",
    "        Sm[:,:m-1]=Sm[:,1:m];\n",
    "        Ym[:,:m-1]=Ym[:,1:m];\n",
    "        Sm[:,m-1] = s0;\n",
    "        Ym[:,m-1] = y0;    \n",
    "        # never forget the minus sign\n",
    "        p = -getHg_lbfgs(g1,Sm,Ym,hdiag);\n",
    "   \n",
    "    ## LINE SEARCH: strong wolfe also provides the new gradient at the new point...\n",
    "    # if we use weak wolfe...then what?\n",
    "    #[alpha ,fs,gs, eval_count]= strongwolfe(f, gradf,p,x1,f1,g1);\n",
    "    # weak_wolfe(f, d, x0, fx0, gx0)\n",
    "    alpha,fs, eval_count =  weak_wolfe(f, p, x1, f1, g1, fac = 0.9);\n",
    "    \n",
    "    print('evals for line search: ', eval_count)\n",
    "    x0 = x1; ## update x0 with x1 the delta\n",
    "    g0 = g1;\n",
    "    \n",
    "    ## make a step\n",
    "    x1 = x1 + alpha*p;\n",
    "    f1 = fs;\n",
    "    #g1 = gs;\n",
    "    # save caculation\n",
    "    # [f1,g1]=feval(myFx,x1);\n",
    "    k = k + 1;\n",
    "    \n",
    "print(f1, f(x1), x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(hdiag, s0,y0, np.outer(s0, y0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHg_lbfgs(g,S,Y,hdiag):\n",
    "    '''\n",
    "        GIVES US APPROXIMATION OF HESSIAN we need to determine next search direction\n",
    "        Input\n",
    "           S:    Memory matrix (n by k) , s{i}=x{i+1}-x{i}\n",
    "           Y:    Memory matrix (n by k) , df{i}=df{i+1}-df{i}\n",
    "           g:    gradient (n by 1)\n",
    "           hdiag value of initial Hessian diagonal elements (scalar)\n",
    "        Output\n",
    "           Hg    the the approximate inverse Hessian multiplied by the gradient g    \n",
    "    '''\n",
    "    n,k = S.shape;\n",
    "    ro = np.zeros(k);\n",
    "    for i in range(k):\n",
    "        ro[i] = 1/np.dot(Y[:,i],S[:,i]);\n",
    "    \n",
    "    q = np.zeros((n,k+1)); #\n",
    "    r = np.zeros(n);   #\n",
    "    \n",
    "    ## \n",
    "    alpha = np.zeros(k);\n",
    "    beta = np.zeros(k);\n",
    "    \n",
    "    ##step 1\n",
    "    q[:,k] = g;\n",
    "\n",
    "    ## first loop\n",
    "    for i in range(k-1, -1, -1):\n",
    "        alpha[i] = ro[i]*np.dot(S[:,i].T, q[:,i+1]);\n",
    "        q[:,i] = q[:,i+1]-alpha[i]*Y[:,i];\n",
    "\n",
    "    ## Multiply by Initial Hessian\n",
    "    ## r should be a vector\n",
    "    r = hdiag@q[:,0];\n",
    "\n",
    "    ## second loop\n",
    "    for i in range(k):\n",
    "        beta[i] = ro[i]*np.dot(Y[:,i].T,r);\n",
    "        r = r + S[:,i]*(alpha[i]-beta[i]);\n",
    "    \n",
    "    Hg=r;\n",
    "    return Hg;\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WOLFE SEARCH INFRASTRUCTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strongwolfe(f,gradf, d, x0, fx0, gx0):\n",
    "    '''\n",
    "        f: function \n",
    "        gradf: gradient of function (autograd)\n",
    "        d: search direction\n",
    "        x0: initial start\n",
    "        fx0: initial function value at x0\n",
    "        gx0: initial gradient at x0; should be a vector\n",
    "        \n",
    "        OUTPUTS:\n",
    "        alphas: step size\n",
    "        fs:     the function value at x0+alphas*d\n",
    "        gs:     the gradient value at x0+alphas*d  \n",
    "    '''\n",
    "    ## only up to 3 iterations allowed?\n",
    "    \n",
    "    maxIter = 3; # how many iterations to run \n",
    "    alpham = 20;\n",
    "    alphap = 0;\n",
    "    c1 = 1e-4;\n",
    "    c2 = 0.9;\n",
    "    \n",
    "    alphax = 1;\n",
    "    gx0 = np.dot(gx0.T,d); ## what's this \n",
    "    fxp = fx0;\n",
    "    gxp = gx0;\n",
    "    i=1;\n",
    "    eval_count = 0;\n",
    "    while(True):\n",
    "        xx = x0 +alphax*d; #proposed new val;\n",
    "        ## evaluate function and gradient\n",
    "        fxx, gxx = f(xx), gradf(xx);\n",
    "        eval_count+=1;\n",
    "        fs = fxx;\n",
    "        gs = gxx;\n",
    "        gxx = np.dot(gxx.T,d); ## project on the direction of search?\n",
    "        if(fxx > fx0 + c1*alphax*gx0 or ((i>1) and (fxx >= fxp))): #evaluate wolfe condition;\n",
    "            [alphas,fs,gs, subc] = Zoom(f,gradf,x0,d,alphap,alphax,fx0,gx0);\n",
    "            eval_count+=subc\n",
    "            break;\n",
    "        if(np.abs(gxx) <= -c2*gx0): #not sure what this condition is\n",
    "            alphas = alphax;\n",
    "            break;\n",
    "        if(gxx >= 0):\n",
    "            [alphas,fs,gs, subc] = Zoom(f,gradf,x0,d,alphax,alphap,fx0,gx0);\n",
    "            eval_count+=subc\n",
    "\n",
    "            break;\n",
    "        ## all breaking conditions were not satisfied\n",
    "        alphap = alphax;\n",
    "        fxp = fxx;\n",
    "        gxp = gxx;\n",
    "        if(i > maxIter):\n",
    "            alphas = alphax;\n",
    "            break\n",
    "        ## r = rand(1);%randomly choose alphax from interval (alphap,alpham)\n",
    "        r = 0.8;\n",
    "        alphax = alphax + (alpham-alphax)*r;\n",
    "        i+=1;\n",
    "    \n",
    "    return alphas, fs, gs, eval_count\n",
    "            \n",
    "def Zoom(f, gradf, x0,d,alphal,alphah,fx0,gx0):\n",
    "    '''\n",
    "        myFx:\n",
    "        x0:\n",
    "        d:\n",
    "        alphal:\n",
    "        alphah:\n",
    "        fx0:\n",
    "        gx0:\n",
    "        \n",
    "        returns\n",
    "            alphas\n",
    "            fs\n",
    "            gs \n",
    "            eval_counts\n",
    "        \n",
    "    '''\n",
    "    c1 = 1e-4;\n",
    "    c2 = 0.9;\n",
    "    i =0;\n",
    "    maxIter = 3\n",
    "    while(True):\n",
    "        ## bisection\n",
    "        alphax = 0.5*(alphal+alphah);\n",
    "        alphas = alphax;\n",
    "        xx = x0 + alphax*d;\n",
    "        ## this is highly inefficient if we need to evaluate grad\n",
    "        [fxx,gxx] = f(xx), gradf(xx);\n",
    "        fs = fxx;\n",
    "        gs = gxx;\n",
    "        gxx = np.dot(gxx.T,d);\n",
    "        xl = x0 + alphal*d;\n",
    "        \n",
    "        fxl = f(xl); #no gradient required\n",
    "        \n",
    "        if((fxx > fx0 + c1*alphax*gx0) or (fxx >= fxl)):\n",
    "            alphah = alphax;\n",
    "        else:\n",
    "            if(abs(gxx) <= -c2*gx0):\n",
    "                alphas = alphax;\n",
    "                break;\n",
    "            if(gxx*(alphah-alphal) >= 0):\n",
    "                alphah = alphal;\n",
    "            alphal = alphax;\n",
    "       \n",
    "        i = i+1;\n",
    "        if(i > maxIter):\n",
    "            alphas = alphax;\n",
    "        \n",
    "    return alphas,fs,gs, i\n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_wolfe(f, d, x0, fx0, gx0, fac = 0.5):\n",
    "    '''\n",
    "        backtracking line search\n",
    "        does not evaluate gradient\n",
    "        there is no updated gradient gs...\n",
    "    '''\n",
    "    alphaMax     = 1; #this is the maximum step length\n",
    "    alpha        = alphaMax;\n",
    "    #fac          = .75; #< 1 reduction factor of alpha\n",
    "    c_1          = 1e-1; #\n",
    "    eps=1e-4\n",
    "    \n",
    "    eval_count = 1;\n",
    "    fs = f(x0+alpha*d)\n",
    "    while(fs > fx0 + c_1*alpha*(np.dot(d.T, gx0))):\n",
    "\n",
    "        alpha = fac*alpha;\n",
    "        eval_count+=1;\n",
    "        fs = f(x0+alpha*d);\n",
    "        if alpha < 10*eps:\n",
    "            break;\n",
    "            #error('Error in Line search - alpha close to working precision');\n",
    "      \n",
    "    return alpha,fs, eval_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## quadratic line search?\n",
    "# function alpha = mb_quadraticApproximationLineSearch(objFunc,objFuncValue,x,dx,dir)\n",
    "\n",
    "# % 2010 m.bangert@dkfz.de\n",
    "# % line search using a quadratic approximation we are setting to minimum of\n",
    "# % the quadratic function phi(t) = a*t^2+b*t+c which is determined by three \n",
    "# % points: objFunc(x), gradFunc'(x) and objFunc(x+alpha*dir)\n",
    "# %\n",
    "# % objFunc      - handle for objective function\n",
    "# % objFuncValue - current objective function value @ x\n",
    "# % x            - x\n",
    "# % dx           - dx\n",
    "# % dir          - search direction\n",
    "# %\n",
    "# % example : mb_backtrackingLineSearch(objFunc,objFuncValue,x,dx,dir)\n",
    "\n",
    "# alpha = 1.5;\n",
    "\n",
    "# c     = objFuncValue;\n",
    "# b     = (dir'*dx);\n",
    "# a     = ( objFunc(x+alpha*dir) - b*alpha - c) / ...\n",
    "#              alpha^2;\n",
    "        \n",
    "# alpha = - b / (2*a);\n",
    "\n",
    "# numOfQuadApprox = 0;\n",
    "# c_1             = 1e-1;\n",
    "# % check if armijo criterion fullfilled\n",
    "# while objFunc(x+alpha*dir) > objFuncValue + c_1*alpha*dir'*dx;\n",
    "\n",
    "#     numOfQuadApprox = numOfQuadApprox + 1;\n",
    "    \n",
    "#     a     = ( objFunc(x+alpha*dir) - b*alpha - c) / ...\n",
    "#              alpha^2;\n",
    "#     alpha = - b / (2*a);\n",
    "\n",
    "#     if numOfQuadApprox > 10\n",
    "#         warning(['Error in Line search - quadraric approximation failed more than 10 times\\n' ...\n",
    "#                  'Starting backtracking line search\\n']);\n",
    "#         alpha = mb_backtrackingLineSearch(objFunc,objFuncValue,x,dx,dir);\n",
    "#         return;             \n",
    "#     end\n",
    "    \n",
    "# end\n",
    "\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
